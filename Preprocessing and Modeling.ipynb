{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d259df57-986a-40e8-b3e6-a5fdc657b467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+--------------------+------------------+\n|experience_level|company_size_category|            position|        prediction|\n+----------------+---------------------+--------------------+------------------+\n|     Entry-Level|                    S|--Software Engine...|117515.18209311957|\n|     Entry-Level|                    S|AI Scientist at R...|117515.18209311957|\n|     Entry-Level|                    S|AWS /DevOps Engineer|117515.18209311957|\n|     Entry-Level|                    S|Air management sp...|117515.18209311957|\n|     Entry-Level|                    S|             Analyst|117515.18209311957|\n|     Entry-Level|                    S|             Analyst|117515.18209311957|\n|     Entry-Level|                    S|             Analyst|117515.18209311957|\n|     Entry-Level|                    S|      Analyst at Abc|117515.18209311957|\n|     Entry-Level|                    M|Analytics Enginee...|142859.65785400855|\n|     Entry-Level|                    S|   Applied Scientist|141046.08321699995|\n|     Entry-Level|                    S|   Applied Scientist|141046.08321699995|\n|     Entry-Level|                    S|   Applied Scientist|141046.08321699995|\n|     Entry-Level|                    S|Applied/Data Scie...|117515.18209311957|\n|     Entry-Level|                    S|Area Manager, Dat...|117515.18209311957|\n|     Entry-Level|                    S|Associate Operati...|117515.18209311957|\n|     Entry-Level|                    S|Associate Princip...|117515.18209311957|\n|     Entry-Level|                    S|      Audio Engineer|117515.18209311957|\n|     Entry-Level|                    S|      Audio Engineer|117515.18209311957|\n|     Entry-Level|                    S|      Audio Engineer|117515.18209311957|\n|     Entry-Level|                    S|BI Data Analyst @...|117515.18209311957|\n+----------------+---------------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, StructType, StructField, BooleanType, FloatType, ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, when, lit, to_json, split, element_at, count, lower, trim, explode, from_json, sum as _sum,\n",
    "    regexp_extract, create_map, levenshtein\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, VectorAssembler, OneHotEncoder, StopWordsRemover, Tokenizer, NGram, HashingTF,\n",
    "    MinHashLSH, RegexTokenizer, SQLTransformer\n",
    ")\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from json.decoder import JSONDecodeError\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Salary Predictor\").getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "jobs_in_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/sewar.hino@campus.technion.ac.il/jobs_in_data.csv\")\n",
    "\n",
    "scrapped_big_companies = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/sewar.hino@campus.technion.ac.il/LinkedIn_people_profiles_BIG_COMPANIES_ONLY.csv\")\n",
    "\n",
    "job_titles = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/sewar.hino@campus.technion.ac.il/titles.csv\")\n",
    "\n",
    "companies = spark.read.parquet('/linkedin/companies')\n",
    "\n",
    "profiles = spark.read.parquet('/linkedin/people')\n",
    "\n",
    "extra_profiles = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/sewar.hino@campus.technion.ac.il/scrabbed_profiles.csv\")\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# preapare data sets\n",
    "# jobs_in_data = jobs_in_data.withColumnRenamed(\"employee_residence\", \"location__DUPLICATE\")\n",
    "\n",
    "profiles = profiles.withColumnRenamed(\"current_company:company_id\", \"current_company_company_id\")\n",
    "profiles = profiles.withColumnRenamed(\"current_company:name\", \"current_company_name\")\n",
    "profiles = profiles.withColumnRenamed(\"city\", \"location__DUPLICATE\")\n",
    "columns_of_interest = ['current_company_company_id', 'id', 'position', 'about','current_company','experience','location__DUPLICATE','current_company_name', 'country_code']  \n",
    "\n",
    "\n",
    "profiles1 = profiles.select(columns_of_interest) # Original data\n",
    "extra_profiles1 = extra_profiles.select(columns_of_interest) # Scraped data\n",
    "\n",
    "profiles1 = profiles1.withColumn(\"is_original_profile\", lit(1))\n",
    "extra_profiles1 = extra_profiles1.withColumn(\"is_original_profile\", lit(0))\n",
    "\n",
    "profiles1 = profiles1.dropDuplicates(['id'])\n",
    "extra_profiles1 = extra_profiles1.dropDuplicates(['id'])\n",
    "\n",
    "profiles1 = profiles1.withColumn(\"current_company\", to_json(\"current_company\"))\n",
    "profiles1 = profiles1.withColumn(\"experience\", to_json(\"experience\"))\n",
    "\n",
    "combined_profiles = profiles1.unionByName(extra_profiles1, allowMissingColumns=True)\n",
    "\n",
    "##########################################################################\n",
    "# Employee Residence\n",
    "\n",
    "# Define a mapping dictionary for country codes to country names\n",
    "country_code_mapping = {\n",
    "    \"LT\": \"Lithuania\",\n",
    "    \"FI\": \"Finland\",\n",
    "    \"UA\": \"Ukraine\",\n",
    "    \"RO\": \"Romania\",\n",
    "    \"NL\": \"Netherlands\",\n",
    "    \"PL\": \"Poland\",\n",
    "    \"AT\": \"Austria\",\n",
    "    \"RU\": \"Russia\",\n",
    "    \"HR\": \"Croatia\",\n",
    "    \"CZ\": \"Czech Republic\",\n",
    "    \"PT\": \"Portugal\",\n",
    "    \"BY\": \"Belarus\",\n",
    "    \"MD\": \"Moldova\",\n",
    "    \"DE\": \"Germany\",\n",
    "    \"ES\": \"Spain\",\n",
    "    \"ME\": \"Montenegro\",\n",
    "    \"RS\": \"Serbia\",\n",
    "    \"FR\": \"France\",\n",
    "    \"CH\": \"Switzerland\",\n",
    "    \"GR\": \"Greece\",\n",
    "    \"US\": \"United States\"\n",
    "}\n",
    "\n",
    "# Convert the mapping dictionary to a Spark DataFrame\n",
    "country_mapping_df = spark.createDataFrame(country_code_mapping.items(), [\"country_code\", \"country_name\"])\n",
    "\n",
    "# Join the sampled DataFrame with the country mapping DataFrame to get the country names\n",
    "df_with_country_names = combined_profiles.join(country_mapping_df, on=\"country_code\", how=\"left\")\n",
    "\n",
    "# Use when() to handle missing values and fill them with \"Unknown\"\n",
    "df_with_country_names = df_with_country_names.withColumn(\"country_name\", \n",
    "                                                         when(col(\"country_name\").isNull(), \"Unknown\")\n",
    "                                                         .otherwise(col(\"country_name\")))\n",
    "\n",
    "# Extract the distinct values from the 'employee_residence' column in 'jobs_in_data'\n",
    "employee_residence_values = jobs_in_data.select('employee_residence').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter 'df_with_country_names' based on 'country_name' being in the values list of 'employee_residence' in 'jobs_in_data'\n",
    "filtered_df = df_with_country_names.filter(col('country_name').isin(employee_residence_values))\n",
    "\n",
    "# Rename the column country_name to employee_residence\n",
    "filtered_df = filtered_df.withColumnRenamed(\"country_name\", \"employee_residence\")\n",
    "\n",
    "##########################################################################\n",
    "# # keep profiles realted to Data \n",
    "\n",
    "# Rename 'id' column\n",
    "filtered_df = filtered_df.withColumnRenamed(\"id\", \"profile_id\")\n",
    "\n",
    "# List of keywords related to the data domain\n",
    "data_domain_keywords = [\"data\", \"analyst\", \"scientist\", \"engineer\", \"machine learning\", \"AI\", \"big data\"]\n",
    "\n",
    "# Define a function to check if a profile is related to the data domain\n",
    "def is_related_to_data_domain(position):\n",
    "    for keyword in data_domain_keywords:\n",
    "        if keyword.lower() in position.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Register the UDF\n",
    "is_related_udf = udf(is_related_to_data_domain, BooleanType())\n",
    "\n",
    "# Apply the UDF to filter out unrelated profiles\n",
    "filtered_df = filtered_df.filter(is_related_udf(col(\"position\")))\n",
    "\n",
    "# Pipeline for creating vectors of job_title in jobs_in_data\n",
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(job_title) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(jobs_in_data)\n",
    "\n",
    "result_jobs = model.transform(jobs_in_data)\n",
    "result_jobs = result_jobs.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "\n",
    "# Pipeline for creating vectors of position in filtered_df\n",
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(position) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(filtered_df)\n",
    "\n",
    "result_filtered = model.transform(filtered_df)\n",
    "result_filtered = result_filtered.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "\n",
    "# Filter out rows with null values in the 'position' column\n",
    "result_filtered = result_filtered.filter(col(\"position\").isNotNull())\n",
    "\n",
    "# Perform the similarity join operation\n",
    "result = model.stages[-1].approxSimilarityJoin(result_filtered, result_jobs, 0.5, \"jaccardDist\")\n",
    "\n",
    "# Select and sort the relevant columns\n",
    "result = (result\n",
    "          .select('datasetA.profile_id', 'datasetA.position', 'datasetB.job_title', 'jaccardDist')\n",
    "          .sort(col('datasetA.position')))\n",
    "\n",
    "w = Window.partitionBy('profile_id')\n",
    "result = (result\n",
    "           .withColumn('minDist', F.min('jaccardDist').over(w))\n",
    "           .where(F.col('jaccardDist') == F.col('minDist'))\n",
    "           .drop('minDist'))\n",
    "(result\n",
    " .select('position', 'job_title', 'jaccardDist')\n",
    " .sort(F.col('profile_id'))).dropDuplicates()\n",
    "\n",
    "result = result.withColumnRenamed('profile_id', 'result_profile_id').withColumnRenamed('position', 'result_position')\n",
    "\n",
    "# Join filtered_df with result based on the position column\n",
    "filtered_df = filtered_df.join(result, filtered_df.position == result.result_position, \"inner\")\n",
    "\n",
    "filtered_df = filtered_df.dropDuplicates()\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def clean_company_name(name):\n",
    "    if name is not None:\n",
    "        return re.sub(r'[\\W_]+', '', name.lower()).strip()\n",
    "    return None\n",
    "\n",
    "clean_company_name_udf = udf(clean_company_name, StringType())\n",
    "\n",
    "filtered_profiles = filtered_df.withColumn('cleaned_company_name', clean_company_name_udf(col('current_company_name')))\n",
    "companies = companies.withColumn('cleaned_name', clean_company_name_udf(col('name')))\n",
    "\n",
    "joined_prof = filtered_profiles.join(companies, filtered_profiles['cleaned_company_name'] == companies['cleaned_name'], 'inner')\n",
    "\n",
    "\n",
    "filtered_profiles = joined_prof\n",
    "##########################################################################\n",
    "# comapny size \n",
    "\n",
    "# UDF to extract and categorize company size\n",
    "def categorize_company_size(size):\n",
    "    if not size:\n",
    "        return None\n",
    "    num_match = re.findall(r'\\d+', size)\n",
    "    num_match = [int(num) for num in num_match]\n",
    "    if not num_match:\n",
    "        return None\n",
    "\n",
    "    max_size = max(map(int, num_match))\n",
    "    if max_size <= 50:\n",
    "        return 'S'\n",
    "    elif max_size <= 250:\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'L'\n",
    "\n",
    "categorize_company_size_udf = udf(categorize_company_size, StringType())\n",
    "\n",
    "profiles_with_company_size = filtered_profiles.withColumn(\n",
    "    \"company_size_category\",\n",
    "    categorize_company_size_udf(filtered_profiles[\"company_size\"])\n",
    ")\n",
    "\n",
    "filtered_profiles = profiles_with_company_size\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# parse 'duration_short' into total years\n",
    "@udf(FloatType())\n",
    "def parse_duration_short_udf(duration_str):\n",
    "    import re \n",
    "    if not duration_str:\n",
    "        return 0.0\n",
    "    \n",
    "    # Regex to extract years and months\n",
    "    years = re.search(r'(\\d+)\\s+year', duration_str)\n",
    "    months = re.search(r'(\\d+)\\s+month', duration_str)\n",
    "    \n",
    "    year_value = int(years.group(1)) if years else 0\n",
    "    month_value = int(months.group(1)) if months else 0\n",
    "    \n",
    "    return year_value + month_value / 12.0\n",
    "\n",
    "experience_schema = ArrayType(StructType([\n",
    "    StructField(\"duration_short\", StringType(), True),\n",
    "]))\n",
    "\n",
    "filtered_profiles = filtered_profiles.withColumn(\"experience_details\", from_json(\"experience\", experience_schema))\n",
    "filtered_profiles = filtered_profiles.withColumn(\"single_experience\", explode(\"experience_details\"))\n",
    "filtered_profiles = filtered_profiles.withColumn(\"years\", parse_duration_short_udf(col(\"single_experience.duration_short\")))\n",
    "\n",
    "@udf(StringType())\n",
    "def categorize_experience(years):\n",
    "    if years <= 2:\n",
    "        return 'Entry-Level'\n",
    "    elif years <= 5:\n",
    "        return 'Mid-Level'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "    \n",
    "\n",
    "total_experience = filtered_profiles.groupBy(filtered_profiles[\"profile_id\"]) \\\n",
    "    .agg(_sum(\"years\").alias(\"total_experience_years\"))\n",
    "\n",
    "filtered_profiles = filtered_profiles.join(total_experience, \"profile_id\", \"left\")\n",
    "\n",
    "filtered_profiles = filtered_profiles.withColumn(\"experience_level\", categorize_experience(col(\"total_experience_years\")))\n",
    "\n",
    "filtered_profiles = filtered_profiles.dropDuplicates(['experience_level', 'position','company_size'])\n",
    "\n",
    "jobs_in_data = jobs_in_data.withColumn(\"salary_in_usd\", col(\"salary_in_usd\").cast(FloatType()))\n",
    "jobs_in_data = jobs_in_data.withColumnRenamed(\"job_title\", \"position\")\n",
    "jobs_in_data = jobs_in_data.withColumnRenamed(\"company_size\", \"company_size_category\")\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "stages = []\n",
    "\n",
    "# Indexing and encoding for \"experience_level\", \"company_size\", \"position\"\n",
    "for feature_name in [\"experience_level\", \"company_size_category\", \"position\"]:\n",
    "    indexer = StringIndexer(inputCol=feature_name, outputCol=f\"{feature_name}_index\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{feature_name}_index\"], outputCols=[f\"{feature_name}_vec\"])\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# Vector assembler\n",
    "assembler = VectorAssembler(inputCols=[f\"{feature_name}_vec\" for feature_name in [\"experience_level\", \"company_size_category\", \"position\"]], outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"salary_in_usd\")\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"salary_in_usd\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# stages += [rf]\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"salary_in_usd\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Using RandomForestRegressor in the pipeline for demonstration\n",
    "pipeline = Pipeline(stages=stages + [rf])\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "experience_levels = jobs_in_data.select(\"experience_level\").distinct().count()\n",
    "\n",
    "if experience_levels > 1:\n",
    "\n",
    "    cvModel = crossval.fit(jobs_in_data)\n",
    "    bestModel = cvModel.bestModel\n",
    "    predictions = bestModel.transform(filtered_profiles)\n",
    "    predictions = predictions.dropDuplicates(['experience_level', 'position','company_size'])\n",
    "    predictions.select(\"experience_level\", \"company_size_category\", \"position\", \"prediction\").show()\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient diversity in 'experience_level'.\")\n",
    "\n",
    "##############################################################################\n",
    "# spark.stop()\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "(train_data, test_data) = jobs_in_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Predict and evaluate on test data which must contain 'salary_in_usd'\n",
    "predictions = cvModel.bestModel.transform(test_data)\n",
    "\n",
    "# Evaluators for RMSE, MAE, and R²\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"salary_in_usd\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"salary_in_usd\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"salary_in_usd\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Coefficient of Determination (R²): {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
